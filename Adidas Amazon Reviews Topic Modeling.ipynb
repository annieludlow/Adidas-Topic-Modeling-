{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load file in as a read file \n",
    "loadedjson = open('meta_Clothing_Shoes_and_Jewelry.json', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "allproducts = {}\n",
    "listofcategories = {}\n",
    "#go thorugh each line at a time (count 1, is going through one line of the data\n",
    "#dividing count by 10000 with no remainder then print it)\n",
    "for aline in loadedjson:\n",
    "    count += 1 \n",
    "    if count % 100000 == 0:\n",
    "        print(count)\n",
    "    aproduct = eval(aline)\n",
    "    \n",
    "    allproducts[aproduct['asin']] = aproduct\n",
    "\n",
    "    for categories in aproduct['categories']:\n",
    "        for acategory in categories:\n",
    "            if acategory in listofcategories:\n",
    "                listofcategories[acategory] += 1\n",
    "            if acategory not in listofcategories:\n",
    "                listofcategories[acategory] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracts asins \n",
    "count = 0 \n",
    "alladidasasins = set()\n",
    "for aproduct in allproducts:\n",
    "    theproduct = allproducts[aproduct]\n",
    "    count += 1\n",
    "    if count % 100000 == 0:\n",
    "        print (count/1503384)\n",
    "    for categories in theproduct['categories']:\n",
    "        for acategory in categories:\n",
    "             if 'adidas' in acategory.lower():\n",
    "                 alladidasasins.add(theproduct['asin'])\n",
    "\n",
    "print (alladidasasins)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#upload reviews data set \n",
    "loadedjson = open('reviews_Clothing_Shoes_and_Jewelry.json', 'r') \n",
    "allreviews = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "#extract reviews\n",
    "for aline in loadedjson:\n",
    "    count += 1 \n",
    "    if count % 100000 == 0:\n",
    "        print(count)\n",
    "    areview = eval(aline)\n",
    "    theasin = areview['asin']\n",
    "    thereviewer = areview['reviewerID']\n",
    "    \n",
    "    if theasin in alladidasasins:\n",
    "        thekey = '%s.%s' % (theasin,thereviewer)\n",
    "        allreviews[thekey] = areview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-dd3b3f788031>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mallreviews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'alladidasreviews.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mallreviews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'alladidasreviews.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'stopwords'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "# all adidas reviews into json file \n",
    "json.dump(allreviews, open('alladidasreviews.json', 'w'))\n",
    "allreviews = json.load(open('alladidasreviews.json', 'r'))\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stopwords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-23d90baaacb6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#implement stop words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mstop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'adidas'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stopwords' is not defined"
     ]
    }
   ],
   "source": [
    "#implement stop words\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.append('adidas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create 2 sets for good and bad reviews for the model\n",
    "#good reviews\n",
    "goodreviews = set()\n",
    "def load_texts(topicdata):\n",
    "    for areview in topicdata:\n",
    "        if 'reviewText' in topicdata[areview]:\n",
    "            #reviews that are under 2 stars\n",
    "            if int(topicdata[areview]['overall']) >4:    \n",
    "                reviewtext = topicdata[areview] ['reviewText']\n",
    "                date = topicdata[areview]['reviewTime']\n",
    "                datenocomma = date.replace(\",\",\"\")\n",
    "                monthdateyear = datenocomma.split(' ')     \n",
    "                if monthdateyear[2] == '2013':\n",
    "                    summary = topicdata[areview]['summary']\n",
    "                    asin = topicdata[areview]['asin']\n",
    "                    review = '%s %s %s' % (asin, summary, reviewtext)\n",
    "                    goodreviews.add(review)\n",
    "                    \n",
    "load_texts(allreviews)\n",
    "gooddocuments = list(goodreviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TfidfVectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-79a6e9c6ae72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#create matrix with vectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgooddocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TfidfVectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "#create matrix with vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
    "X = vectorizer.fit_transform(gooddocuments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K Means Model \n",
    "#True k is number of topic models \n",
    "true_k = 8\n",
    "#clusters is number of models \n",
    "model = KMeans (n_clusters=true_k, max_iter=100000)\n",
    "#fit model. X is matrix of data \n",
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prints topics\n",
    "print('Top terms per cluster for good reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#looking in the model at the clusters and printing the order or the clusters based on their distances\n",
    "order_centroids = model.cluster_centers_.argsort()[:,::-1]\n",
    "terms = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(true_k):\n",
    "    topic_terms = [terms[ind] for ind in order_centroids[i,:4]]\n",
    "    print('%d: %s' % (i, ' '.join(topic_terms)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bad reviews set \n",
    "badreviews = set()\n",
    "def load_texts(topicdata):\n",
    "    for areview in topicdata:\n",
    "        if 'reviewText' in topicdata[areview]:\n",
    "            #reviews that are under 2 stars\n",
    "            if int(topicdata[areview]['overall']) <2:    \n",
    "                reviewtext = topicdata[areview] ['reviewText']\n",
    "                date = topicdata[areview]['reviewTime']\n",
    "                datenocomma = date.replace(\",\",\"\")\n",
    "                monthdateyear = datenocomma.split(' ')     \n",
    "                if monthdateyear[2] == '2014':\n",
    "                    summary = topicdata[areview]['summary']\n",
    "                    asin = topicdata[areview]['asin']\n",
    "                    review = '%s %s %s' % (asin, summary, reviewtext)\n",
    "                    badreviews.add(review)\n",
    "\n",
    "load_texts(allreviews)\n",
    "baddocuments = list(badreviews)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
    "X = vectorizer.fit_transform(baddocuments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#topic models \n",
    "true_k = 8\n",
    "# clusters is number of models \n",
    "model = KMeans (n_clusters=true_k, max_iter=100000)\n",
    "#fit model. X is matrix of data \n",
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print topics\n",
    "print('Top terms per cluster for bad reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking in the model at the clusters and printing the order or the clusters based on their distances\n",
    "order_centroids = model.cluster_centers_.argsort()[:,::-1]\n",
    "terms = vectorizer.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(true_k):\n",
    "    topic_terms = [terms[ind] for ind in order_centroids[i,:4]]\n",
    "    print('%d: %s' % (i, ' '.join(topic_terms)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "outfiles = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.mkdir('output')\n",
    "except OSError:\n",
    "    print ('directory already exists')\n",
    "else:\n",
    "    print ('successfully created the directory')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating output files to join topic terms together   \n",
    "for atopic in range(true_k):\n",
    "    topicterms = [terms[ind] for ind in order_centroids[atopic, :4]]\n",
    "    outfiles[atopic] = open(os.path.join('output', '_'.join(topicterms)+ '.txt'), 'w')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for areview in allreviews:\n",
    "    if 'reviewText' in allreviews[areview]:\n",
    "        thereview = allreviews[areview]\n",
    "        review = '%s %s %s' % (thereview['asin'], thereview['summary'], thereview['reviewText'])\n",
    "        Y = vectorizer.transform([review])\n",
    "       \n",
    "        for prediction in model.predict(Y):\n",
    "            outfiles[prediction].write('%s\\n' % review)\n",
    "\n",
    "for n, f in outfiles.items():\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<string>, line 1)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m2910\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-b61b6b29f08c>\"\u001b[0;36m, line \u001b[0;32m15\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    aproduct = eval(aline)\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"<string>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    {'asin': 'B00AFZJZ38', 'title': 'Reebok Womens ZigKick Alpha Running Shoe', 'imUrl': 'http://ecx.images-amazon.com/images/I/416%2BvlPUjHL._SX395_.jpg', 'related': {'also_bought': ['B00DCVUEDM', 'B00DCVUEEG', 'B00ATY63IK', 'B00C3B7KW0', 'B00ATY64PC', 'B00E1BSDN0', 'B00E1BSDOE', 'B00AHWC92S', 'B00C3B2U0C', 'B007ZLAMDW', 'B00ATY624K', 'B00ATY5YKS', 'B00AU0LML6', 'B00BK37U9A', 'B008K18490', 'B00DORP7Z4', 'B003TOGAGC', 'B00C2O4JFE', 'B00ATY63JY', 'B002NEGI3E', 'B00ATY629K', 'B006UD4KW0', 'B005Q81TRE', 'B00CMC8M8Q', 'B0080CD0TI', 'B00C6C83K4', 'B00ATY6038', 'B006WWXATI', 'B008K18904', 'B007YZBKZ8', 'B0075IRTVI', 'B00AHWMCSO', 'B006WWNRCS', 'B00BFAXOI4', 'B00A4L2W4M', 'B0014JFFZ6', 'B006RBH2PW'], 'also_viewed': ['B00DCVUEDM', 'B00C3B7KW0', 'B00ATY64PC', 'B00ATY63IK', 'B007ZLAMDW', 'B00E1BSDOE', 'B008K18850', 'B00C3B7I2C', 'B00C3B7I68', 'B00AHWMHR0', 'B00DCVUEEG', 'B00ATY624K', 'B006UD4KW0', 'B007ZLALEM', 'B00F1AY7XK', 'B00AHWMK7C', 'B00ATY63JY', 'B005C2P31W', 'B00ATY64OI', 'B005C321U2', 'B00C3B7H00', 'B00BWQ03NU', 'B008K18904', 'B00DORPB5U', 'B00ATY629K', 'B00CTI1TR4', 'B005Q81TRE', 'B006UCA7Q4', 'B008K18490', 'B006WWNRCS', 'B008MJFBGE', 'B00E1BSDN0', 'B00BWMPHWQ', 'B00ATY64MK', 'B008N6SW8U', 'B00ATY6038', 'B008K1885K', 'B00CTI1TYW', 'B00HNAO3F8', 'B003GDJQBM', 'B00FJ213FK', 'B00C3B7KSY', 'B00AHWMF54', 'B00BKW4V7U', 'B006W3S1V4', 'B006UD7S60', 'B00C3B7KWU', 'B00ATY5YKS', 'B00ATY6268', 'B00HBDDG50', 'B00E1BSDL2', 'B00GGR9H9Q', 'B004IPPSLE', 'B006UDJGEW', 'B00DORP7Z4', 'B00CTI1TM4', 'B00C3B7JMQ', 'B00DORPBDM'], 'bought_together': ['B00DCVUEEG', 'B00ATY63IK', 'B00DCVUEDM', 'B00C3B7KW0']}, 'salesRank': {'Shoes': 33288}, 'categories': [['Clothing, Shoes & Jewelry', 'Shoes & Accessories: International Shipping Available'], ['Clothing, Shoes & Jewelry', 'R', 'Reebok'], ['Clothing, Shoes & Jewelry', 'Women', 'Shoes', 'Athletic', 'Running'], ['Clothing, Shoes & Jewelry', 'Novelty, Co\u001b[0m\n\u001b[0m                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
